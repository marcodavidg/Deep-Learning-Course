{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.12\n",
      "Tensorflow version: 2.3.0\n",
      "Keras version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the sequence length for training data\n",
    "seq_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 unique characters\n"
     ]
    }
   ],
   "source": [
    "text = open('shakespeare_train.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# Sort the unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the text\n",
    "Methods to convert the strings to a numerical representation and the other way around. This section also creates sections of text to define training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create layer to go from characters to ID's. The input of the layer is the output from tf.strings.unicode_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create layer and method to go from ID's to characters, the reverse process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide training data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (201,), types: tf.int64>\n",
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "b\"know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be\"\n",
      "b' done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but'\n",
      "b' the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to parti'\n",
      "b'cularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecon'\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training pairs\n",
    "Function to make pairs of a letter and the rest that come after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of these pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 200), (64, 200)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = (512)\n",
    "\n",
    "# Simple RNN (True) or LSTM (False)\n",
    "train_simpleRNN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model with and without LSTM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnnLayer = tf.keras.layers.SimpleRNN(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.rnnLayer.get_initial_state(x)\n",
    "        x, states = self.rnnLayer(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class MyModelLSTM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnnLayer = tf.keras.layers.LSTM(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.rnnLayer.get_initial_state(x)\n",
    "        x, states1, states2 = self.rnnLayer(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states1, states2\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 200, 68) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"my_model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     multiple                  17408     \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     multiple                  393728    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  34884     \n",
      "=================================================================\n",
      "Total params: 446,020\n",
      "Trainable params: 446,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelSimple = MyModel(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = modelSimple(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "modelSimple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 68) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"my_model_lstm_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      multiple                  17408     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                multiple                  18882560  \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  139332    \n",
      "=================================================================\n",
      "Total params: 19,039,300\n",
      "Trainable params: 19,039,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelLSTM = MyModelLSTM(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = modelLSTM(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "modelLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model without LSTM\n"
     ]
    }
   ],
   "source": [
    "# Model to use\n",
    "try:\n",
    "    model = modelSimple\n",
    "    using_LSTM = False\n",
    "    print(\"Using model without LSTM\")\n",
    "except:\n",
    "    model = modelLSTM\n",
    "    using_LSTM = True\n",
    "    print(\"Using model with LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "The Sparse Categorical Crossentropy serves the same purpose as the Bits per Character, that is why it is used here in the loss for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 02:47:34.633338: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2021-12-28 02:47:34.633435: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1513] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "if using_LSTM:\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"LSTM - Sequence:\" + str(seq_length) + \" RNN Units: \" + str(rnn_units) + \"- %Y%m%d-%H%M%S\")\n",
    "else:\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"Sequence:\" + str(seq_length) + \" RNN Units: \" + str(rnn_units) + \"- %Y%m%d-%H%M%S\")\n",
    "\n",
    "# Tensorboard config\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training without interruptions to get plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "  2/673 [..............................] - ETA: 1:03 - loss: 4.1929 - accuracy: 0.0898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 03:09:14.239871: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2021-12-27 03:09:14.360249: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1513] CUPTI activity buffer flushed\n",
      "2021-12-27 03:09:14.367487: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 582 callback api events and 582 activity events. \n",
      "2021-12-27 03:09:14.395376: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14\n",
      "2021-12-27 03:09:14.411434: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14/lami.trace.json.gz\n",
      "2021-12-27 03:09:14.423471: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14\n",
      "2021-12-27 03:09:14.424244: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14/lami.memory_profile.json.gz\n",
      "2021-12-27 03:09:14.424704: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14Dumped tool data for xplane.pb to logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14/lami.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14/lami.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14/lami.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14/lami.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/LSTM - Sequence:100 RNN Units: 2048- 20211227-030912/train/plugins/profile/2021_12_27_03_09_14/lami.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 55s 81ms/step - loss: 2.0043 - accuracy: 0.4274\n",
      "Epoch 2/15\n",
      "673/673 [==============================] - 55s 82ms/step - loss: 1.3920 - accuracy: 0.5734\n",
      "Epoch 3/15\n",
      "673/673 [==============================] - 55s 82ms/step - loss: 1.2736 - accuracy: 0.6026\n",
      "Epoch 4/15\n",
      "673/673 [==============================] - 56s 84ms/step - loss: 1.2104 - accuracy: 0.6189\n",
      "Epoch 5/15\n",
      "673/673 [==============================] - 56s 84ms/step - loss: 1.1587 - accuracy: 0.6330\n",
      "Epoch 6/15\n",
      "673/673 [==============================] - 56s 83ms/step - loss: 1.1075 - accuracy: 0.6478\n",
      "Epoch 7/15\n",
      "673/673 [==============================] - 56s 82ms/step - loss: 1.0522 - accuracy: 0.6639\n",
      "Epoch 8/15\n",
      "673/673 [==============================] - 56s 83ms/step - loss: 0.9926 - accuracy: 0.6819\n",
      "Epoch 9/15\n",
      "673/673 [==============================] - 55s 82ms/step - loss: 0.9319 - accuracy: 0.7008\n",
      "Epoch 10/15\n",
      "673/673 [==============================] - 55s 82ms/step - loss: 0.8739 - accuracy: 0.7192\n",
      "Epoch 11/15\n",
      "673/673 [==============================] - 55s 82ms/step - loss: 0.8245 - accuracy: 0.7346\n",
      "Epoch 12/15\n",
      "673/673 [==============================] - 55s 82ms/step - loss: 0.7851 - accuracy: 0.7467\n",
      "Epoch 13/15\n",
      "673/673 [==============================] - 55s 81ms/step - loss: 0.7539 - accuracy: 0.7559\n",
      "Epoch 14/15\n",
      "673/673 [==============================] - 55s 81ms/step - loss: 0.7350 - accuracy: 0.7611\n",
      "Epoch 15/15\n",
      "673/673 [==============================] - 55s 82ms/step - loss: 0.7239 - accuracy: 0.7640\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        # - Low temperatures results in more predictable text.\n",
    "        # - Higher temperatures results in more surprising text.\n",
    "\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent the unknown \"[UNK]\" token from being generated in the text.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf weight at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, using_LSTM, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Get the model's predictions\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        \n",
    "        if using_LSTM:\n",
    "            predicted_logits, states1, states2 = self.model(inputs=input_ids, states=states,\n",
    "                                                  return_state=True)\n",
    "            states_return = [states1, states2]\n",
    "        else:\n",
    "            predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                                  return_state=True)\n",
    "            states_return = states\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        \n",
    "        # Apply the prediction mask for the unknown token\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get output with a specific input. For the final section of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIETA:\n",
      "If I can hear by day and night to supper.\n",
      "Now, for an ush? Timorad within your sight,\n",
      "Weak washed for your own desert. Let not, even for\n",
      "Is past; but in the host, to death, are run.\n",
      "\n",
      "PORTIA:\n",
      "The webst and mirth o' the kingdom you shall love:\n",
      "And what this goodly doctor, in Simphrook,\n",
      "The Turk, that gave these two and wind, behold\n",
      "Our Gaunts' estimpatives, out, ovidges down\n",
      "And digg'd his thoughts how to cut off a heart?\n",
      "Or rather, shall I lack a week with you;\n",
      "and having that such softer after such\n",
      "As when we had our kind to embark so long,\n",
      "Is almost friendship: six ye hope for the time,\n",
      "Whiles he may conquer fortune and reward.\n",
      "\n",
      "EXETER:\n",
      "That is most faint. As black as I have ever committed.\n",
      "\n",
      "JULIA:\n",
      "You shall be revolved if your willship that shall\n",
      "beat you. Yet your cook, it was by that will she ackees.\n",
      "\n",
      "PISTOL:\n",
      "The score of court, is yet merely fought?\n",
      "\n",
      "Second Merchant:\n",
      "He did; did I nay in his pride deny\n",
      "us the drink? Is there any man's heart?\n",
      "\n",
      "ROMEO:\n",
      "If he be her, now the pledges  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.575113534927368\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['JULIETA:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, using_LSTM, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR LSTM\n",
    "for epoch in range(EPOCHS):\n",
    "    model.fit(dataset, epochs=1)\n",
    "    if (epoch == 1 or epoch == 4 or epoch == 8 or epoch == 12 or epoch == 15):\n",
    "        print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "        states = None\n",
    "        next_char = tf.constant(['First Citizen:\\nBefore we proceed any further,'])\n",
    "        result = [next_char]\n",
    "\n",
    "        for n in range(1000):\n",
    "            next_char, states = one_step_model.generate_one_step(next_char, using_LSTM, states=states)\n",
    "            result.append(next_char)\n",
    "\n",
    "        result = tf.strings.join(result)\n",
    "        end = time.time()\n",
    "        print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338/338 [==============================] - 16s 46ms/step - loss: 2.2834 - accuracy: 0.3594\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.7891 - accuracy: 0.4726\n",
      "Generating text after epoch: 1\n",
      "First Citizen:\n",
      "Before we proceed any further,\n",
      "But the ood, whice her and langers.\n",
      "\n",
      "IOWER:\n",
      "Hes, we'll cemest mades armites\n",
      "I stand may store\n",
      "Bring grocker ead, this bear stornced-pirgaich.\n",
      "\n",
      "AucI at your father\n",
      "Haph betaly pus,\n",
      "So spito much monemal.\n",
      "\n",
      "MICK:\n",
      "Her, Gies in the caint of the calls and bauth. I work's heart\n",
      "Deas tike, And of I joy! decome me new in thuse\n",
      "Caseet as this!\n",
      "\n",
      "FOUD:\n",
      "God yet I have weach, unwing as you, deach so I way.\n",
      "\n",
      "SICOLEEL:\n",
      "Calm, that:\n",
      "Tran, I calive 'trick it: his ploverond dast, if thou thinr your paincaso ouths prrengy, ans I one fill he will and none. Gotss, Lood patcus's marnical haughter's mistserp\n",
      "sit; you worment gold fross dreet,\n",
      "Trurther's meancered,\n",
      "Letweel we lann, on Vile; I name to have deacr's meatalk\n",
      "Sometherch'd to my life his us, come, with All dos, I pay soinour feint,\n",
      "Ar a Hore;\n",
      "Allind the commance is tay:\n",
      "And with sigh to bruded\n",
      "And thise death and glain he then shase eapine enne to nemes King thyours! my lord; canestire,\n",
      "This in the maid\n",
      "Of thee, MyselfI's and field plains for with B \n",
      "\n",
      "________________________________________________________________________________\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 1.6234 - accuracy: 0.5165\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.5366 - accuracy: 0.5392\n",
      "338/338 [==============================] - 15s 46ms/step - loss: 1.4849 - accuracy: 0.5523\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.4494 - accuracy: 0.5608\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.4236 - accuracy: 0.5670\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.4036 - accuracy: 0.5721\n",
      "Generating text after epoch: 7\n",
      "First Citizen:\n",
      "Before we proceed any further,\n",
      "How should stay' the peosomes\n",
      "Shall save him to strength up.\n",
      "\n",
      "APHELIA:\n",
      "Is childron much mournets shall?\n",
      "Did your kindly tawn an your satury!\n",
      "\n",
      "BIRON:\n",
      "Command, a parts?\n",
      "I' thir unto the formaw should be to-morrow upon\n",
      "On each may paper of the witch: 'I mustress;\n",
      "And to my face.\n",
      "\n",
      "EMILIA:\n",
      "What erusy, my lord, Pene; the cleir's king,\n",
      "My mothouse foul king, be sits do withalt it of\n",
      "Resolved, I say?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Then, then, modeat friends,--\n",
      "\n",
      "MINGRIUS:\n",
      "Barraments, most fleeting so\n",
      "much shadows thy father\n",
      "Like him to make me have portune!\n",
      "If my where then-gatting I hid of our arms,\n",
      "And comes for matter\n",
      "Enticeniwe roses,\n",
      "Pardon them art thou?\n",
      "\n",
      "MARK ANTONY:\n",
      "There as shall the fools, sir?\n",
      "\n",
      "LAURCEL:\n",
      "An you so!\n",
      "\n",
      "PAROLLES:\n",
      "I help to Behelman's life: anly, madam, you make!\n",
      "Strikes a vile in the carries thy bearty?\n",
      "\n",
      "ParelyiK:\n",
      "I say, I think.\n",
      "\n",
      "CLEOPATRA:\n",
      "The matter!\n",
      "\n",
      "CALIBAN:\n",
      "Never see you\n",
      "To the countrabe on\n",
      "most? And muldived, my causand prizes,\n",
      "And yet her said!\n",
      "\n",
      "BASSANIO:\n",
      "Fishall, I dare so in \n",
      "\n",
      "________________________________________________________________________________\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3878 - accuracy: 0.5755\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3760 - accuracy: 0.5782\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3625 - accuracy: 0.5816\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3534 - accuracy: 0.5839\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.3450 - accuracy: 0.5857\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.3375 - accuracy: 0.5876\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3311 - accuracy: 0.5892\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3256 - accuracy: 0.5902\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3191 - accuracy: 0.5920\n",
      "Generating text after epoch: 16\n",
      "First Citizen:\n",
      "Before we proceed any further, or no, nay,\n",
      "Now for you all; This is Captor: is 'twere friendly\n",
      "As partly; the have sorrows always we'll report: 'tis good treture, thou be asquire a\n",
      "Rrown's affection thou bearest it.\n",
      "\n",
      "Nurse:\n",
      "Now, by the pursued men'st, and the number naturer\n",
      "That we will, and run out a Wantenolate dreams steel; doth he\n",
      "which cartier with my face, in hollow.\n",
      "\n",
      "CORIOLANUS:\n",
      "You know, read.\n",
      "\n",
      "Jowdine:\n",
      "Which, Warwick, to that; for thy shown\n",
      "The gentlemen From at person from your goodness,\n",
      "Holds his shadow through the ladies.\n",
      "\n",
      "CARDINAL WOLSEY:\n",
      "The naw fasting to-night: you, the other. Come, and my temper to you; and he shall Reward Percy,\n",
      "It would have shoption to the king.\n",
      "The jewel:--O, 'sware Marcius! God favouration lamentity;\n",
      "As groans that we say thy life\n",
      "Should feel tweath in winges? Farewell.\n",
      "\n",
      "First Geapted,\n",
      "Encellow, yea, as the vices, either corrept these favourer that might into white\n",
      "Who, before his head, swallow'd to do\n",
      "In sincable ensue him\n",
      "That they come not with speech\n",
      "such a men of marriage \n",
      "\n",
      "________________________________________________________________________________\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3142 - accuracy: 0.5933\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3106 - accuracy: 0.5940\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.3055 - accuracy: 0.5955\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.3016 - accuracy: 0.5965\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.2983 - accuracy: 0.5971\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.2957 - accuracy: 0.5979\n",
      "Generating text after epoch: 22\n",
      "First Citizen:\n",
      "Before we proceed any further, good counsels:\n",
      "An't that caves the appleous fiend and grave-all;\n",
      "And go as we lived, ere\n",
      "you do see yoursel. Trop alone;\n",
      "Whose does hold you\n",
      "Looks dead, the world with Frenchmal and fight? Hectors. Her, that's would\n",
      "have a lion's wife nearer.\n",
      "\n",
      "Lord Chief-JOHSTONEG:\n",
      "How now, Sir John I warrant her founded out\n",
      "The ansista days and sisters. It is night's action\n",
      "Keeps the court of Caesar?\n",
      "\n",
      "IACHIMO:\n",
      "But I protest it was led too light and by lieuping monerulous.\n",
      "\n",
      "PRINCE HENRY:\n",
      "Well, I am amazed: leave the commission.\n",
      "\n",
      "Second Lord:\n",
      "It is purse-wisdom to buy how not;\n",
      "And in question good windren of the moet\n",
      "That rasca say it be seen to make a gaught belongs.\n",
      "\n",
      "SPEED:\n",
      "' was with the abuned cries the deugter hath stood.\n",
      "Upon my weapon, Nercation that?\n",
      "\n",
      "SUFFOLK:\n",
      "Is i' the conveniance of the foe;\n",
      "And that nights here's the master;\n",
      "And we'll call by giver your wife i' fantasy! will my remembrance.\n",
      "\n",
      "OTHELLO:\n",
      "I will begain you beathed hiving;\n",
      "Like that my services.\n",
      "\n",
      "KING RICHARD II:\n",
      "Good gods, or law \n",
      "\n",
      "________________________________________________________________________________\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.2917 - accuracy: 0.5990\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.2894 - accuracy: 0.5994\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.2859 - accuracy: 0.6004\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.2834 - accuracy: 0.6009\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.2810 - accuracy: 0.6017\n",
      "338/338 [==============================] - 16s 46ms/step - loss: 1.2788 - accuracy: 0.6023\n",
      "338/338 [==============================] - 16s 47ms/step - loss: 1.2770 - accuracy: 0.6026\n",
      "Generating text after epoch: 29\n",
      "First Citizen:\n",
      "Before we proceed any further, if you will run\n",
      "the harsh by their sinsult hived to pray.\n",
      "\n",
      "LAUNCE:\n",
      "Sir, soft, if you do marry thee,\n",
      "The queen: you go off,'d thy life.\n",
      "By my birds to his fair men, only shaff! and the foul heir with death.\n",
      "\n",
      "BENEDICK:\n",
      "Father, a word in hopination!\n",
      "What should run here oncels he\n",
      "is to render thee thee is, like an things thy claimbels,\n",
      "That he has twelve dow of privately.\n",
      "\n",
      "MACULIO:\n",
      "\n",
      "HOLOFERNES:\n",
      "He comes: who hath unto the fortunes thrive.\n",
      "\n",
      "PRINCESS:\n",
      "A half, the sea of victors thither:\n",
      "By that Leona dust I heard him!\n",
      "\n",
      "TIMON:\n",
      "Trust me the little oath such as the humour of,\n",
      "Not a naught to tavell'd in Alioleon: tell me\n",
      "in our cock.\n",
      "\n",
      "LAUNCE:\n",
      "How he could not have made them.\n",
      "\n",
      "BALTHAZAR:\n",
      "Why I humbly mellet, betom?\n",
      "\n",
      "PRINCESS:\n",
      "A wonder. Harry, but thou shalt in an\n",
      "offencisu\n",
      "Thy darted retired from the pight of point. How is A merry thanks from thee!\n",
      "As thought me man froth with him; and I do bulled,\n",
      "Offection was the like that bred it at;\n",
      "And, that breaking his eye, and cry outrull,\n",
      "Let thee no \n",
      "\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# FOR THE NORMAL MODEL\n",
    "for epoch in range(EPOCHS):\n",
    "    model.fit(dataset, epochs=1)\n",
    "    if (epoch == 1 or epoch == 7 or epoch == 16 or epoch == 22 or epoch == 29):\n",
    "        print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "        states = None\n",
    "        next_char = tf.constant(['First Citizen:\\nBefore we proceed any further,'])\n",
    "        result = [next_char]\n",
    "\n",
    "        for n in range(1000):\n",
    "            next_char, states = one_step_model.generate_one_step(next_char, False, states=states)\n",
    "            result.append(next_char)\n",
    "\n",
    "        result = tf.strings.join(result)\n",
    "        end = time.time()\n",
    "        print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "myenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
