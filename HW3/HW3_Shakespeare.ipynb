{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d99516",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8e2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d702f15",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdaf9c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 unique characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open('shakespeare_train.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd7bec",
   "metadata": {},
   "source": [
    "# Process the text\n",
    "Methods to convert the strings to a numerical representation and the other way around. This section also creates sections of text to define training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aee2d5",
   "metadata": {},
   "source": [
    "Create layer to go from characters to ID's. The input of the layer is the output from tf.strings.unicode_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3d135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f7266",
   "metadata": {},
   "source": [
    "Create layer and method to go from ID's to characters, the reverse process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aaf47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db19762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9b4ec",
   "metadata": {},
   "source": [
    "Divide training data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67341acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ae10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568207c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (101,), types: tf.int64>\n",
      "b'First Citizen:\\r\\nBefore we proceed any further, hear me speak.\\r\\n\\r\\nAll:\\r\\nSpeak, speak.\\r\\n\\r\\nFirst Citizen'\n",
      "b':\\r\\nYou are all resolved rather to die than to famish?\\r\\n\\r\\nAll:\\r\\nResolved. resolved.\\r\\n\\r\\nFirst Citizen:\\r'\n",
      "b\"\\nFirst, you know Caius Marcius is chief enemy to the people.\\r\\n\\r\\nAll:\\r\\nWe know't, we know't.\\r\\n\\r\\nFirst \"\n",
      "b\"Citizen:\\r\\nLet us kill him, and we'll have corn at our own price.\\r\\nIs't a verdict?\\r\\n\\r\\nAll:\\r\\nNo more ta\"\n",
      "b\"lking on't; let it be done: away, away!\\r\\n\\r\\nSecond Citizen:\\r\\nOne word, good citizens.\\r\\n\\r\\nFirst Citizen\"\n"
     ]
    }
   ],
   "source": [
    "# Actual examples of chunks of text\n",
    "print(sequences)\n",
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20004d2a",
   "metadata": {},
   "source": [
    "### Create training pairs\n",
    "Function to make pairs of a letter and the one that comes after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef4147dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e9590ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\r\\nBefore we proceed any further, hear me speak.\\r\\n\\r\\nAll:\\r\\nSpeak, speak.\\r\\n\\r\\nFirst Citize'\n",
      "Target: b'irst Citizen:\\r\\nBefore we proceed any further, hear me speak.\\r\\n\\r\\nAll:\\r\\nSpeak, speak.\\r\\n\\r\\nFirst Citizen'\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401389b",
   "metadata": {},
   "source": [
    "### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1373c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f303a41",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6a052e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb890942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.SimpleRNN(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e40025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c83818c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 69) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37cbf96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  17664     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       multiple                  1311744   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  70725     \n",
      "=================================================================\n",
      "Total params: 1,400,133\n",
      "Trainable params: 1,400,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf9f301e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60,  8, 14, 38, 67, 61, 67, 60, 13, 15, 21, 41, 33,  8, 45,  0, 15,\n",
       "       62, 46, 26, 27, 27, 48, 38, 47,  3,  8,  6, 15, 29, 61, 28,  7, 64,\n",
       "        8, 35, 57, 24, 58, 12, 62,  3, 55, 23, 13, 55, 14,  6, 23, 65, 33,\n",
       "       51, 61, 43, 49, 10, 10, 39, 64, 37, 10, 67, 14, 33, 65, 49, 19, 58,\n",
       "       35, 35, 13,  8, 20, 60, 67, 14, 12, 18, 49,  6, 42, 11, 30,  0, 18,\n",
       "       27, 60, 11, 16, 25, 47, 16, 15, 20, 24,  2, 28, 29, 61, 47],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "214275af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'e glorious gods sit in hourly synod about thy\\r\\nparticular prosperity, and love thee no worse than\\r\\nt'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"r,?Xysyr;AG[S,c[UNK]AtdLMMfXe ,&AOsN'v,UoJp:t mI;m?&IwSisag..YvW.y?SwgEpUU;,Fry?:Dg&]3P[UNK]DMr3BKeBAFJ\\rNOse\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a3bf0",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79295c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4772bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "697/697 [==============================] - 172s 239ms/step - loss: 2.0563\n",
      "Epoch 2/20\n",
      "697/697 [==============================] - 236s 327ms/step - loss: 1.5383\n",
      "Epoch 3/20\n",
      "408/697 [================>.............] - ETA: 1:29 - loss: 1.4243"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32099233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f94fd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deafaf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\r\n",
      "I know us out 'twixt his unclean in death.\r\n",
      "\r\n",
      "TALBOT:\r\n",
      "Go to, England! Tell me, befall it: the true\r\n",
      "issuir: the true't master do the cruelty.\r\n",
      "\r\n",
      "MIRANDA:\r\n",
      "O, pray, give me what doctor, your son's daughter.\r\n",
      "\r\n",
      "CLOTEN:\r\n",
      "I love those bond.\r\n",
      "\r\n",
      "TYBELLOTO:\r\n",
      "To the king.\r\n",
      "\r\n",
      "PANDARUS:\r\n",
      "Mistress Ford! a girdle, chamorous enemy!\r\n",
      "\r\n",
      "ARIEL:\r\n",
      "\r\n",
      "GLOUCESTER:\r\n",
      "Now he is vouch'd into Posthumus,\r\n",
      "They two are twenty tooldersing bed.\r\n",
      "Marry, sir?\r\n",
      "\r\n",
      "CARDINAL WOLSEY:\r\n",
      "What trust?\r\n",
      "\r\n",
      "PANDARUS:\r\n",
      "We are lady at the beasts. What were good choice of it\r\n",
      "with you, and could they creph quick again, as it be,\r\n",
      "That in this sele-a field but majesty\r\n",
      "To mongrel? Let it all dream'd like a Glendic;\r\n",
      "Who bear his boots within: I see you were\r\n",
      "As doctor, would be shorted, lads of darkness,\r\n",
      "Yet laugh'd in to answer, glide: where are you?\r\n",
      "\r\n",
      "COUNTESS:\r\n",
      "Nay, heaven can make condemn enough to work\r\n",
      "Reputation? show deward and starve: stealth,\r\n",
      "Intion! Say'st thou?\r\n",
      "\r\n",
      "KING HENRY V:\r\n",
      "Pray, sir, art thou not?\r\n",
      "\r\n",
      "DICK:\r\n",
      "I  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 8.287822246551514\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c56cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
